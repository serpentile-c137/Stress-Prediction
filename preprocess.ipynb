{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pandas\n",
    "import numpy as np\n",
    "import os\n",
    "# import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S02', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S03', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S04', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S05', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S06', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S07', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S08', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S09', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S10', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S11', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S12', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S13', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S14', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S15', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S16', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S17', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S18', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S19', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S20', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S21', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S22', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S23', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S24', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S25', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S26', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S27', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S28', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S29', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S30', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S31', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S32', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S33', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S34', 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data\\\\S35']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:16: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\GANESH\\AppData\\Local\\Temp\\ipykernel_16916\\3744578695.py:16: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  directory_path = 'E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data'  # Replace with your directory path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_all_folders(directory):\n",
    "    # List to store all folder names\n",
    "    folders = []\n",
    "    \n",
    "    # Walk through all directories and subdirectories\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for dirname in dirnames:\n",
    "            # Append the folder name to the list\n",
    "            folders.append(os.path.join(dirpath, dirname))\n",
    "    \n",
    "    return folders\n",
    "\n",
    "# Example usage\n",
    "directory_path = 'E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data'  # Replace with your directory path\n",
    "folders = get_all_folders(directory_path)\n",
    "print(folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def read_and_combine_multiple_prefix_csv(folders, output_file, prefixes):\n",
    "    \"\"\"\n",
    "    Reads CSV files from multiple folders that match any of the given prefixes and combines them into a new CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - folders: List of folder paths to search for CSV files.\n",
    "    - output_file: Path of the new CSV file to save combined data.\n",
    "    - prefixes: List of prefixes to match in the filenames (e.g., ['data_', 'sales_']).\n",
    "    \"\"\"\n",
    "    # Create an empty list to store DataFrames\n",
    "    data_frames = []\n",
    "\n",
    "    # Iterate through each folder\n",
    "    for folder in folders:\n",
    "        # Loop through each prefix in the list\n",
    "        for prefix in prefixes:\n",
    "            # Construct the search pattern for the current prefix\n",
    "            search_pattern = os.path.join(folder, f\"{prefix}*.csv\")\n",
    "\n",
    "            # Use glob to find CSV files that match the current pattern\n",
    "            csv_files = glob.glob(search_pattern)\n",
    "\n",
    "            # Loop through each file\n",
    "            for file in csv_files:\n",
    "                try:\n",
    "                    # Read the CSV file into a DataFrame\n",
    "                    df = pd.read_csv(file, header=1)\n",
    "                    data_frames.append(df)  # Add the DataFrame to the list\n",
    "                    print(f\"Successfully read {file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read {file}: {e}\")\n",
    "\n",
    "    # If there are no CSV files found or read, return early\n",
    "    if not data_frames:\n",
    "        print(\"No CSV files found matching the prefixes.\")\n",
    "        return\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    # Write the combined DataFrame to a new CSV file\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"Combined CSV saved as {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "# folders = ['folder1', 'folder2', 'folder3']  # List of folders to search in\n",
    "output_file = 'combined_output.csv'  # Path to the new combined CSV file\n",
    "\n",
    "# List of prefixes to match, e.g., 'data_' and 'sales_'\n",
    "prefixes = ['HR', 'BVP', 'TEMP', 'EDA', 'IBI']\n",
    "\n",
    "read_and_combine_multiple_prefix_csv(folders, output_file, prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       EDA_1644227574.000000  TEMP_1644227574.000000  IBI_1644227574.000000  \\\n",
      "0                   4.000000                    4.00             604.000000   \n",
      "1                   0.000000                   34.79             604.937500   \n",
      "2                   0.622764                   34.79             605.687500   \n",
      "3                   0.759875                   34.79             609.562500   \n",
      "4                   0.883157                   34.79             644.765625   \n",
      "...                      ...                     ...                    ...   \n",
      "14260               0.025628                   30.67                    NaN   \n",
      "14261               0.025628                   30.81                    NaN   \n",
      "14262               0.026910                   30.81                    NaN   \n",
      "14263                    NaN                   30.81                    NaN   \n",
      "14264                    NaN                   30.81                    NaN   \n",
      "\n",
      "       IBI_ IBI  \n",
      "0      0.687500  \n",
      "1      0.937500  \n",
      "2      0.750000  \n",
      "3      0.859375  \n",
      "4      0.875000  \n",
      "...         ...  \n",
      "14260       NaN  \n",
      "14261       NaN  \n",
      "14262       NaN  \n",
      "14263       NaN  \n",
      "14264       NaN  \n",
      "\n",
      "[14265 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\GANESH\\AppData\\Local\\Temp\\ipykernel_18324\\3968099352.py:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  eda_df = pd.read_csv('E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S02\\EDA.csv')\n",
      "C:\\Users\\GANESH\\AppData\\Local\\Temp\\ipykernel_18324\\3968099352.py:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  temp_df = pd.read_csv('E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S02\\TEMP.csv')\n",
      "C:\\Users\\GANESH\\AppData\\Local\\Temp\\ipykernel_18324\\3968099352.py:5: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  ibi_df = pd.read_csv('E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S02\\IBI.csv')\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the CSV files\n",
    "# E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S02\n",
    "eda_df = pd.read_csv('E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S02\\EDA.csv')\n",
    "temp_df = pd.read_csv('E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S02\\TEMP.csv')\n",
    "ibi_df = pd.read_csv('E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S02\\IBI.csv')\n",
    "\n",
    "# Step 2: Add prefixes to columns to indicate their origin\n",
    "eda_df.columns = ['EDA_' + col for col in eda_df.columns]\n",
    "temp_df.columns = ['TEMP_' + col for col in temp_df.columns]\n",
    "ibi_df.columns = ['IBI_' + col for col in ibi_df.columns]\n",
    "\n",
    "# Step 3: Combine the DataFrames (assuming they have the same number of rows)\n",
    "combined_df = pd.concat([eda_df, temp_df, ibi_df], axis=1)\n",
    "\n",
    "# Step 4: Save the combined DataFrame to a new CSV\n",
    "combined_df.to_csv('combined_output.csv', index=False)\n",
    "\n",
    "# Optional: Display the combined DataFrame to verify\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Readme as it's not a folder.\n",
      "Processing folder: S01\n",
      "Saved combined data for folder S01 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S01\\S01_combined_output.csv\n",
      "Processing folder: S02\n",
      "Saved combined data for folder S02 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S02\\S02_combined_output.csv\n",
      "Processing folder: S03\n",
      "Saved combined data for folder S03 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S03\\S03_combined_output.csv\n",
      "Processing folder: S04\n",
      "Saved combined data for folder S04 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S04\\S04_combined_output.csv\n",
      "Processing folder: S05\n",
      "Saved combined data for folder S05 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S05\\S05_combined_output.csv\n",
      "Processing folder: S06\n",
      "Saved combined data for folder S06 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S06\\S06_combined_output.csv\n",
      "Processing folder: S07\n",
      "Saved combined data for folder S07 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S07\\S07_combined_output.csv\n",
      "Processing folder: S08\n",
      "Saved combined data for folder S08 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S08\\S08_combined_output.csv\n",
      "Processing folder: S09\n",
      "Saved combined data for folder S09 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S09\\S09_combined_output.csv\n",
      "Processing folder: S10\n",
      "Saved combined data for folder S10 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S10\\S10_combined_output.csv\n",
      "Processing folder: S11\n",
      "Saved combined data for folder S11 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S11\\S11_combined_output.csv\n",
      "Processing folder: S12\n",
      "Saved combined data for folder S12 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S12\\S12_combined_output.csv\n",
      "Processing folder: S13\n",
      "Saved combined data for folder S13 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S13\\S13_combined_output.csv\n",
      "Processing folder: S14\n",
      "Saved combined data for folder S14 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S14\\S14_combined_output.csv\n",
      "Processing folder: S15\n",
      "Saved combined data for folder S15 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S15\\S15_combined_output.csv\n",
      "Processing folder: S16\n",
      "Saved combined data for folder S16 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S16\\S16_combined_output.csv\n",
      "Processing folder: S17\n",
      "Saved combined data for folder S17 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S17\\S17_combined_output.csv\n",
      "Processing folder: S18\n",
      "Saved combined data for folder S18 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S18\\S18_combined_output.csv\n",
      "Processing folder: S19\n",
      "Saved combined data for folder S19 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S19\\S19_combined_output.csv\n",
      "Processing folder: S20\n",
      "Saved combined data for folder S20 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S20\\S20_combined_output.csv\n",
      "Processing folder: S21\n",
      "Saved combined data for folder S21 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S21\\S21_combined_output.csv\n",
      "Processing folder: S22\n",
      "Saved combined data for folder S22 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S22\\S22_combined_output.csv\n",
      "Processing folder: S23\n",
      "Saved combined data for folder S23 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S23\\S23_combined_output.csv\n",
      "Processing folder: S24\n",
      "Saved combined data for folder S24 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S24\\S24_combined_output.csv\n",
      "Processing folder: S25\n",
      "Saved combined data for folder S25 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S25\\S25_combined_output.csv\n",
      "Processing folder: S26\n",
      "Saved combined data for folder S26 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S26\\S26_combined_output.csv\n",
      "Processing folder: S27\n",
      "Saved combined data for folder S27 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S27\\S27_combined_output.csv\n",
      "Processing folder: S28\n",
      "Saved combined data for folder S28 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S28\\S28_combined_output.csv\n",
      "Processing folder: S29\n",
      "Saved combined data for folder S29 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S29\\S29_combined_output.csv\n",
      "Processing folder: S30\n",
      "Saved combined data for folder S30 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S30\\S30_combined_output.csv\n",
      "Processing folder: S31\n",
      "Saved combined data for folder S31 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S31\\S31_combined_output.csv\n",
      "Processing folder: S32\n",
      "Saved combined data for folder S32 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S32\\S32_combined_output.csv\n",
      "Processing folder: S33\n",
      "Saved combined data for folder S33 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S33\\S33_combined_output.csv\n",
      "Processing folder: S34\n",
      "Saved combined data for folder S34 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S34\\S34_combined_output.csv\n",
      "Processing folder: S35\n",
      "Saved combined data for folder S35 as E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data\\S35\\S35_combined_output.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_and_combine_csv_in_folders(base_dir):\n",
    "    \"\"\"\n",
    "    Processes all folders in the given directory, loading specific CSV files,\n",
    "    adding prefixes to columns, and combining them into a single CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - base_dir: The base directory containing subfolders with the CSV files.\n",
    "    \"\"\"\n",
    "    # Iterate through each folder in the base directory\n",
    "    for folder_name in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder_name)\n",
    "\n",
    "        # Check if it's a folder (skip files)\n",
    "        if os.path.isdir(folder_path):\n",
    "            print(f\"Processing folder: {folder_name}\")\n",
    "            \n",
    "            # Define paths for the CSV files in the current folder\n",
    "            eda_file = os.path.join(folder_path, 'EDA.csv')\n",
    "            temp_file = os.path.join(folder_path, 'TEMP.csv')\n",
    "            # ibi_file = os.path.join(folder_path, 'IBI.csv')\n",
    "            bvp_file = os.path.join(folder_path, 'BVP.csv')\n",
    "            hr_file = os.path.join(folder_path, 'HR.csv')\n",
    "\n",
    "            # Check if the required files exist\n",
    "            if os.path.exists(eda_file) and os.path.exists(temp_file) and os.path.exists(bvp_file) and os.path.exists(hr_file):\n",
    "                # Step 1: Load the CSV files\n",
    "                eda_df = pd.read_csv(eda_file)\n",
    "                temp_df = pd.read_csv(temp_file)\n",
    "                # ibi_df = pd.read_csv(ibi_file)\n",
    "                bvp_df = pd.read_csv(bvp_file)\n",
    "                hr_df = pd.read_csv(hr_file)\n",
    "\n",
    "                # Step 2: Add prefixes to columns\n",
    "                eda_df.columns = ['EDA_' + col for col in eda_df.columns]\n",
    "                temp_df.columns = ['TEMP_' + col for col in temp_df.columns]\n",
    "                # ibi_df.columns = ['IBI_' + col for col in ibi_df.columns]\n",
    "                bvp_df.columns = ['BVP_' + col for col in bvp_df.columns]\n",
    "                hr_df.columns = ['HR_' + col for col in hr_df.columns]\n",
    "\n",
    "                # Step 3: Combine the DataFrames (assuming they have the same number of rows)\n",
    "                combined_df = pd.concat([eda_df, temp_df,bvp_df,hr_df], axis=1)\n",
    "\n",
    "                # Step 4: Save the combined DataFrame to a new CSV file in the folder\n",
    "                output_file = os.path.join(folder_path, f\"{folder_name}_combined_output.csv\")\n",
    "                combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "                print(f\"Saved combined data for folder {folder_name} as {output_file}\")\n",
    "            else:\n",
    "                print(f\"One or more CSV files are missing in folder {folder_name}. Skipping.\")\n",
    "        else:\n",
    "            print(f\"Skipping {folder_name} as it's not a folder.\")\n",
    "\n",
    "# Example usage\n",
    "base_dir = 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Raw_data'  # Base directory containing the folders\n",
    "process_and_combine_csv_in_folders(base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the root directory containing the subfolders with CSV files\n",
    "root_dir = 'E:\\stress prediction\\Stress-Predict-Dataset\\Raw_data'\n",
    "\n",
    "# List to hold all DataFrames\n",
    "data_frames = []\n",
    "\n",
    "# Use glob to find all CSV files in the subdirectories\n",
    "csv_files = glob.glob(os.path.join(root_dir, '**', '*combined_output.csv'), recursive=True)\n",
    "\n",
    "# Iterate over each CSV file, read it, and append it to the list\n",
    "for csv_file in csv_files:\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(csv_file)\n",
    "        data_frames.append(df)\n",
    "        print(f\"Successfully read: {csv_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {csv_file}: {e}\")\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Define the path to save the combined CSV\n",
    "output_file = 'final_output.csv'\n",
    "\n",
    "# Save the combined DataFrame to a new CSV\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Combined CSV saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S02_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S03_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S04_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S05_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S06_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S07_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S08_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S09_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S10_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S11_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S12_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S13_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S14_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S15_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S16_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S17_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S18_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S19_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S20_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S21_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S22_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S23_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S24_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S25_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S26_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S27_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S28_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S29_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S30_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S31_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S32_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S33_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S34_combined_output.csv\n",
      "Renamed headers and saved E:\\stress prediction\\Stress-Predict-Dataset\\Combined_data\\S35_combined_output.csv\n",
      "Header renaming completed.\n"
     ]
    }
   ],
   "source": [
    "# RENAMING HEADERS\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder containing the CSV files\n",
    "folder_path = 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Combined_data'\n",
    "\n",
    "# List of all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Define the new headers you want to use (ensure the number of new headers matches the number of columns in your CSVs)\n",
    "new_headers = ['EDA', 'id', 'TEMP', 'BVP', 'HR']  # Modify this based on the number of columns in your files\n",
    "\n",
    "# Iterate through each CSV file in the folder\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    \n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Check if the new headers list matches the number of columns in the file\n",
    "    if len(new_headers) == df.shape[1]:\n",
    "        # Rename the columns\n",
    "        df.columns = new_headers\n",
    "        \n",
    "        # Save the DataFrame with new headers to a new CSV file (optional: overwrite original)\n",
    "        new_file_path = os.path.join(folder_path, f\"{csv_file}\")\n",
    "        df.to_csv(new_file_path, index=False)\n",
    "        print(f\"Renamed headers and saved {new_file_path}\")\n",
    "    else:\n",
    "        print(f\"Skipping {csv_file} because the number of columns does not match the new headers list.\")\n",
    "\n",
    "print(\"Header renaming completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file: S02_combined_output.csv\n",
      "Processed file: S03_combined_output.csv\n",
      "Processed file: S04_combined_output.csv\n",
      "Processed file: S05_combined_output.csv\n",
      "Processed file: S06_combined_output.csv\n",
      "Processed file: S07_combined_output.csv\n",
      "Processed file: S08_combined_output.csv\n",
      "Processed file: S09_combined_output.csv\n",
      "Processed file: S10_combined_output.csv\n",
      "Processed file: S11_combined_output.csv\n",
      "Processed file: S12_combined_output.csv\n",
      "Processed file: S13_combined_output.csv\n",
      "Processed file: S14_combined_output.csv\n",
      "Processed file: S15_combined_output.csv\n",
      "Processed file: S16_combined_output.csv\n",
      "Processed file: S17_combined_output.csv\n",
      "Processed file: S18_combined_output.csv\n",
      "Processed file: S19_combined_output.csv\n",
      "Processed file: S20_combined_output.csv\n",
      "Processed file: S21_combined_output.csv\n",
      "Processed file: S22_combined_output.csv\n",
      "Processed file: S23_combined_output.csv\n",
      "Processed file: S24_combined_output.csv\n",
      "Processed file: S25_combined_output.csv\n",
      "Processed file: S26_combined_output.csv\n",
      "Processed file: S27_combined_output.csv\n",
      "Processed file: S28_combined_output.csv\n",
      "Processed file: S29_combined_output.csv\n",
      "Processed file: S30_combined_output.csv\n",
      "Processed file: S31_combined_output.csv\n",
      "Processed file: S32_combined_output.csv\n",
      "Processed file: S33_combined_output.csv\n",
      "Processed file: S34_combined_output.csv\n",
      "Processed file: S35_combined_output.csv\n"
     ]
    }
   ],
   "source": [
    "# ADDING N/A AND TRUNCATING ROWS\n",
    "# Define the folder path where the CSV files are stored\n",
    "folder_path = r'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Combined_data'\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):  # Process only CSV files\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Fill NaN values with \"99999\"\n",
    "        df.fillna(99999, inplace=True)\n",
    "        \n",
    "        # Drop rows with 3 or more \"99999\" values\n",
    "        df = df[df.apply(lambda row: row.isin([99999]).sum() < 3, axis=1)]\n",
    "        \n",
    "        # Save the updated DataFrame back to CSV\n",
    "        df.to_csv(file_path, index=False)\n",
    "        \n",
    "        print(f\"Processed file: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully combined 34 CSV files into 'combined_output.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder containing the CSV files\n",
    "folder_path = 'E:\\\\stress prediction\\\\Stress-Predict-Dataset\\\\Combined_data'\n",
    "\n",
    "# Get list of all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# List to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Variable to track the column count\n",
    "column_count = None\n",
    "\n",
    "# Iterate through all CSV files\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # If it's the first CSV file, store the number of columns\n",
    "    if column_count is None:\n",
    "        column_count = df.shape[1]\n",
    "    \n",
    "    # Check if the current CSV has the same number of columns\n",
    "    if df.shape[1] == column_count:\n",
    "        dfs.append(df)\n",
    "    else:\n",
    "        print(f\"Skipping {csv_file} as it has a different number of columns.\")\n",
    "        \n",
    "# If we have any DataFrames to combine\n",
    "if dfs:\n",
    "    # Combine all DataFrames\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Write the combined DataFrame to a new CSV file\n",
    "    combined_df.to_csv('combined_csv.csv', index=False)\n",
    "    print(f\"Successfully combined {len(dfs)} CSV files into 'combined_output.csv'.\")\n",
    "else:\n",
    "    print(\"No CSV files with the same number of columns were found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('combined_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 451364 entries, 0 to 451363\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   EDA     451364 non-null  float64\n",
      " 1   TEMP    451364 non-null  float64\n",
      " 2   BVP     451364 non-null  float64\n",
      " 3   HR      451364 non-null  float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 13.8 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EDA</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>BVP</th>\n",
       "      <th>HR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.79</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>118.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.622764</td>\n",
       "      <td>34.79</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>113.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.759875</td>\n",
       "      <td>34.79</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>93.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.883157</td>\n",
       "      <td>34.79</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>93.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        EDA   TEMP   BVP      HR\n",
       "0  4.000000   4.00  64.0    1.00\n",
       "1  0.000000  34.79  -0.0  118.00\n",
       "2  0.622764  34.79  -0.0  113.50\n",
       "3  0.759875  34.79  -0.0   93.00\n",
       "4  0.883157  34.79  -0.0   93.25"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EDA</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>BVP</th>\n",
       "      <th>HR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EDA</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000312</td>\n",
       "      <td>-0.007970</td>\n",
       "      <td>0.008453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TEMP</th>\n",
       "      <td>-0.000312</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000801</td>\n",
       "      <td>0.011718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BVP</th>\n",
       "      <td>-0.007970</td>\n",
       "      <td>-0.000801</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HR</th>\n",
       "      <td>0.008453</td>\n",
       "      <td>0.011718</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           EDA      TEMP       BVP        HR\n",
       "EDA   1.000000 -0.000312 -0.007970  0.008453\n",
       "TEMP -0.000312  1.000000 -0.000801  0.011718\n",
       "BVP  -0.007970 -0.000801  1.000000  0.000160\n",
       "HR    0.008453  0.011718  0.000160  1.000000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Readme as it's not a folder.\n",
      "Processing folder: S02\n",
      "Saved combined data for folder S02 as E:\\Shardul\\STRESS\\Raw_data\\S02\\S02_combined_output_ID.csv\n",
      "Processing folder: S03\n",
      "Saved combined data for folder S03 as E:\\Shardul\\STRESS\\Raw_data\\S03\\S03_combined_output_ID.csv\n",
      "Processing folder: S04\n",
      "Saved combined data for folder S04 as E:\\Shardul\\STRESS\\Raw_data\\S04\\S04_combined_output_ID.csv\n",
      "Processing folder: S05\n",
      "Saved combined data for folder S05 as E:\\Shardul\\STRESS\\Raw_data\\S05\\S05_combined_output_ID.csv\n",
      "Processing folder: S06\n",
      "Saved combined data for folder S06 as E:\\Shardul\\STRESS\\Raw_data\\S06\\S06_combined_output_ID.csv\n",
      "Processing folder: S07\n",
      "Saved combined data for folder S07 as E:\\Shardul\\STRESS\\Raw_data\\S07\\S07_combined_output_ID.csv\n",
      "Processing folder: S08\n",
      "Saved combined data for folder S08 as E:\\Shardul\\STRESS\\Raw_data\\S08\\S08_combined_output_ID.csv\n",
      "Processing folder: S09\n",
      "Saved combined data for folder S09 as E:\\Shardul\\STRESS\\Raw_data\\S09\\S09_combined_output_ID.csv\n",
      "Processing folder: S10\n",
      "Saved combined data for folder S10 as E:\\Shardul\\STRESS\\Raw_data\\S10\\S10_combined_output_ID.csv\n",
      "Processing folder: S11\n",
      "Saved combined data for folder S11 as E:\\Shardul\\STRESS\\Raw_data\\S11\\S11_combined_output_ID.csv\n",
      "Processing folder: S12\n",
      "Saved combined data for folder S12 as E:\\Shardul\\STRESS\\Raw_data\\S12\\S12_combined_output_ID.csv\n",
      "Processing folder: S13\n",
      "Saved combined data for folder S13 as E:\\Shardul\\STRESS\\Raw_data\\S13\\S13_combined_output_ID.csv\n",
      "Processing folder: S14\n",
      "Saved combined data for folder S14 as E:\\Shardul\\STRESS\\Raw_data\\S14\\S14_combined_output_ID.csv\n",
      "Processing folder: S15\n",
      "Saved combined data for folder S15 as E:\\Shardul\\STRESS\\Raw_data\\S15\\S15_combined_output_ID.csv\n",
      "Processing folder: S16\n",
      "Saved combined data for folder S16 as E:\\Shardul\\STRESS\\Raw_data\\S16\\S16_combined_output_ID.csv\n",
      "Processing folder: S17\n",
      "Saved combined data for folder S17 as E:\\Shardul\\STRESS\\Raw_data\\S17\\S17_combined_output_ID.csv\n",
      "Processing folder: S18\n",
      "Saved combined data for folder S18 as E:\\Shardul\\STRESS\\Raw_data\\S18\\S18_combined_output_ID.csv\n",
      "Processing folder: S19\n",
      "Saved combined data for folder S19 as E:\\Shardul\\STRESS\\Raw_data\\S19\\S19_combined_output_ID.csv\n",
      "Processing folder: S20\n",
      "Saved combined data for folder S20 as E:\\Shardul\\STRESS\\Raw_data\\S20\\S20_combined_output_ID.csv\n",
      "Processing folder: S21\n",
      "Saved combined data for folder S21 as E:\\Shardul\\STRESS\\Raw_data\\S21\\S21_combined_output_ID.csv\n",
      "Processing folder: S22\n",
      "Saved combined data for folder S22 as E:\\Shardul\\STRESS\\Raw_data\\S22\\S22_combined_output_ID.csv\n",
      "Processing folder: S23\n",
      "Saved combined data for folder S23 as E:\\Shardul\\STRESS\\Raw_data\\S23\\S23_combined_output_ID.csv\n",
      "Processing folder: S24\n",
      "Saved combined data for folder S24 as E:\\Shardul\\STRESS\\Raw_data\\S24\\S24_combined_output_ID.csv\n",
      "Processing folder: S25\n",
      "Saved combined data for folder S25 as E:\\Shardul\\STRESS\\Raw_data\\S25\\S25_combined_output_ID.csv\n",
      "Processing folder: S26\n",
      "Saved combined data for folder S26 as E:\\Shardul\\STRESS\\Raw_data\\S26\\S26_combined_output_ID.csv\n",
      "Processing folder: S27\n",
      "Saved combined data for folder S27 as E:\\Shardul\\STRESS\\Raw_data\\S27\\S27_combined_output_ID.csv\n",
      "Processing folder: S28\n",
      "Saved combined data for folder S28 as E:\\Shardul\\STRESS\\Raw_data\\S28\\S28_combined_output_ID.csv\n",
      "Processing folder: S29\n",
      "Saved combined data for folder S29 as E:\\Shardul\\STRESS\\Raw_data\\S29\\S29_combined_output_ID.csv\n",
      "Processing folder: S30\n",
      "Saved combined data for folder S30 as E:\\Shardul\\STRESS\\Raw_data\\S30\\S30_combined_output_ID.csv\n",
      "Processing folder: S31\n",
      "Saved combined data for folder S31 as E:\\Shardul\\STRESS\\Raw_data\\S31\\S31_combined_output_ID.csv\n",
      "Processing folder: S32\n",
      "Saved combined data for folder S32 as E:\\Shardul\\STRESS\\Raw_data\\S32\\S32_combined_output_ID.csv\n",
      "Processing folder: S33\n",
      "Saved combined data for folder S33 as E:\\Shardul\\STRESS\\Raw_data\\S33\\S33_combined_output_ID.csv\n",
      "Processing folder: S34\n",
      "Saved combined data for folder S34 as E:\\Shardul\\STRESS\\Raw_data\\S34\\S34_combined_output_ID.csv\n",
      "Processing folder: S35\n",
      "Saved combined data for folder S35 as E:\\Shardul\\STRESS\\Raw_data\\S35\\S35_combined_output_ID.csv\n"
     ]
    }
   ],
   "source": [
    "# WITH ID\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_and_combine_csv_in_folders(base_dir):\n",
    "    \"\"\"\n",
    "    Processes all folders in the given directory, loading specific CSV files,\n",
    "    adding prefixes to columns, adding a new 'id' column with the folder name,\n",
    "    and combining them into a single CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - base_dir: The base directory containing subfolders with the CSV files.\n",
    "    \"\"\"\n",
    "    # Iterate through each folder in the base directory\n",
    "    for folder_name in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder_name)\n",
    "\n",
    "        # Check if it's a folder (skip files)\n",
    "        if os.path.isdir(folder_path):\n",
    "            print(f\"Processing folder: {folder_name}\")\n",
    "            \n",
    "            # Define paths for the CSV files in the current folder\n",
    "            eda_file = os.path.join(folder_path, 'EDA.csv')\n",
    "            temp_file = os.path.join(folder_path, 'TEMP.csv')\n",
    "            bvp_file = os.path.join(folder_path, 'BVP.csv')\n",
    "            hr_file = os.path.join(folder_path, 'HR.csv')\n",
    "\n",
    "            # Check if the required files exist\n",
    "            if os.path.exists(eda_file) and os.path.exists(temp_file) and os.path.exists(bvp_file) and os.path.exists(hr_file):\n",
    "                # Step 1: Load the CSV files\n",
    "                eda_df = pd.read_csv(eda_file)\n",
    "                temp_df = pd.read_csv(temp_file)\n",
    "                bvp_df = pd.read_csv(bvp_file)\n",
    "                hr_df = pd.read_csv(hr_file)\n",
    "                df = pd.DataFrame()\n",
    "                # Step 2: Add 'id' column with folder name as value for all rows\n",
    "                # df['id'] = folder_name\n",
    "                # df['id'] = pd.Series(data=folder_name)\n",
    "                eda_df['id'] = folder_name\n",
    "                # bvp_df['id'] = folder_name\n",
    "                # hr_df['id'] = folder_name\n",
    "\n",
    "                # Step 3: Add prefixes to columns\n",
    "                eda_df.columns = ['EDA_' + col for col in eda_df.columns]\n",
    "                temp_df.columns = ['TEMP_' + col for col in temp_df.columns]\n",
    "                bvp_df.columns = ['BVP_' + col for col in bvp_df.columns]\n",
    "                hr_df.columns = ['HR_' + col for col in hr_df.columns]\n",
    "\n",
    "                # Step 4: Combine the DataFrames (assuming they have the same number of rows)\n",
    "                combined_df = pd.concat([eda_df, temp_df, bvp_df, hr_df], axis=1)\n",
    "\n",
    "                # Step 5: Save the combined DataFrame to a new CSV file in the folder\n",
    "                output_file = os.path.join(folder_path, f\"{folder_name}_combined_output_ID.csv\")\n",
    "                combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "                print(f\"Saved combined data for folder {folder_name} as {output_file}\")\n",
    "            else:\n",
    "                print(f\"One or more CSV files are missing in folder {folder_name}. Skipping.\")\n",
    "        else:\n",
    "            print(f\"Skipping {folder_name} as it's not a folder.\")\n",
    "\n",
    "# Example usage\n",
    "base_dir = 'E:\\\\Shardul\\\\STRESS\\\\Raw_data'  # Base directory containing the folders\n",
    "process_and_combine_csv_in_folders(base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GANESH\\AppData\\Local\\Temp\\ipykernel_4492\\3859703347.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file: S07_combined_output_ID.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GANESH\\AppData\\Local\\Temp\\ipykernel_4492\\3859703347.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file: S08_combined_output_ID.csv\n"
     ]
    }
   ],
   "source": [
    "# ADDING N/A AND TRUNCATING ROWS\n",
    "# Define the folder path where the CSV files are stored\n",
    "# folder_path = r'E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID'\n",
    "folder_path = r'E:\\\\Shardul\\\\STRESS\\\\test_id'\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):  # Process only CSV files\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Fill NaN values with \"99999\"\n",
    "        df.fillna(99999, inplace=True)\n",
    "        \n",
    "        # Drop rows with 3 or more \"99999\" values\n",
    "        df = df[df.apply(lambda row: row.isin([99999]).sum() < 3, axis=1)]\n",
    "        \n",
    "        # Save the updated DataFrame back to CSV\n",
    "        df.to_csv(file_path, index=False)\n",
    "        \n",
    "        print(f\"Processed file: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S02_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S03_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S04_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S05_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S06_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S07_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S08_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S09_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S10_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S11_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S12_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S13_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S14_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S15_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S16_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S17_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S18_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S19_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S20_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S21_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S22_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S23_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S24_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S25_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S26_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S27_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S28_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S29_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S30_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S31_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S32_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S33_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S34_combined_output_ID.csv\n",
      "Renamed headers and saved E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID\\S35_combined_output_ID.csv\n",
      "Header renaming completed.\n"
     ]
    }
   ],
   "source": [
    "# RENAMING HEADERS\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder containing the CSV files\n",
    "# folder_path = 'E:\\\\Shardul\\\\STRESS\\\\test_id'\n",
    "folder_path = r'E:\\\\Shardul\\\\STRESS\\\\Combined_data_withID'\n",
    "\n",
    "# List of all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Define the new headers you want to use (ensure the number of new headers matches the number of columns in your CSVs)\n",
    "new_headers = ['id', 'EDA', 'TEMP', 'BVP', 'HR']  # Modify this based on the number of columns in your files\n",
    "\n",
    "# Iterate through each CSV file in the folder\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    \n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Check if the new headers list matches the number of columns in the file\n",
    "    if len(new_headers) == df.shape[1]:\n",
    "        # Rename the columns\n",
    "        df.columns = new_headers\n",
    "        df[['EDA', 'id']] = df[['id', 'EDA']]\n",
    "        \n",
    "        # Save the DataFrame with new headers to a new CSV file (optional: overwrite original)\n",
    "        new_file_path = os.path.join(folder_path, f\"{csv_file}\")\n",
    "        df.to_csv(new_file_path, index=False)\n",
    "        print(f\"Renamed headers and saved {new_file_path}\")\n",
    "    else:\n",
    "        print(f\"Skipping {csv_file} because the number of columns does not match the new headers list.\")\n",
    "\n",
    "print(\"Header renaming completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully combined 13 CSV files into 'final_csv.csv'.\n"
     ]
    }
   ],
   "source": [
    "# combining csv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder containing the CSV files\n",
    "# folder_path = 'E:\\\\Shardul\\\\STRESS\\\\test_id'\n",
    "folder_path = r'C:\\Users\\sahus\\Stress-Prediction\\Combined_data_withID\\New'\n",
    "\n",
    "# Get list of all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# List to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Variable to track the column count\n",
    "column_count = None\n",
    "\n",
    "# Iterate through all CSV files\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # If it's the first CSV file, store the number of columns\n",
    "    if column_count is None:\n",
    "        column_count = df.shape[1]\n",
    "    \n",
    "    # Check if the current CSV has the same number of columns\n",
    "    if df.shape[1] == column_count:\n",
    "        dfs.append(df)\n",
    "    else:\n",
    "        print(f\"Skipping {csv_file} as it has a different number of columns.\")\n",
    "        \n",
    "# If we have any DataFrames to combine\n",
    "if dfs:\n",
    "    # Combine all DataFrames\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Write the combined DataFrame to a new CSV file\n",
    "    combined_df.to_csv(r'C:\\Users\\sahus\\Stress-Prediction\\Combined_data_withID\\New2\\final_csv.csv', index=False)\n",
    "    print(f\"Successfully combined {len(dfs)} CSV files into 'final_csv.csv'.\")\n",
    "else:\n",
    "    print(\"No CSV files with the same number of columns were found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully combined 11 CSV files into 'final_csv.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder containing the CSV files\n",
    "# folder_path = 'E:\\\\Shardul\\\\STRESS\\\\test_id'\n",
    "folder_path = r'C:\\Users\\sahus\\Stress-Prediction\\Combined_data_withID\\New3'\n",
    "\n",
    "# Get list of all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# List to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Variable to track the column count\n",
    "column_count = None\n",
    "\n",
    "# Iterate through all CSV files\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # If it's the first CSV file, store the number of columns\n",
    "    if column_count is None:\n",
    "        column_count = df.shape[1]\n",
    "    \n",
    "    # Check if the current CSV has the same number of columns\n",
    "    if df.shape[1] == column_count:\n",
    "        dfs.append(df)\n",
    "    else:\n",
    "        print(f\"Skipping {csv_file} as it has a different number of columns.\")\n",
    "        \n",
    "# If we have any DataFrames to combine\n",
    "if dfs:\n",
    "    # Combine all DataFrames\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Write the combined DataFrame to a new CSV file\n",
    "    combined_df.to_csv(r'C:\\Users\\sahus\\Stress-Prediction\\Combined_data_withID\\New3\\final_csv.csv', index=False)\n",
    "    print(f\"Successfully combined {len(dfs)} CSV files into 'final_csv.csv'.\")\n",
    "else:\n",
    "    print(\"No CSV files with the same number of columns were found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EDA</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>BVP</th>\n",
       "      <th>HR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S02</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.79</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>118.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S02</td>\n",
       "      <td>0.622764</td>\n",
       "      <td>34.79</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>113.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S02</td>\n",
       "      <td>0.759875</td>\n",
       "      <td>34.79</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>93.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S02</td>\n",
       "      <td>0.883157</td>\n",
       "      <td>34.79</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>93.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id       EDA   TEMP   BVP      HR\n",
       "0  S02  4.000000   4.00  64.0    1.00\n",
       "1  S02  0.000000  34.79  -0.0  118.00\n",
       "2  S02  0.622764  34.79  -0.0  113.50\n",
       "3  S02  0.759875  34.79  -0.0   93.00\n",
       "4  S02  0.883157  34.79  -0.0   93.25"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.read_csv(r'C:\\Users\\sahus\\Stress-Prediction\\Combined_data_withID\\New3\\final_csv.csv')\n",
    "final_df.head()                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id      0.000000\n",
       "EDA     0.000000\n",
       "TEMP    0.000328\n",
       "BVP     0.000000\n",
       "HR      0.750678\n",
       "dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REPLACE 99999 by N/A for simplifiaction\n",
    "final_df.replace(99999.0, 'na', inplace=True)\n",
    "na_count = (final_df == 'na').mean()\n",
    "na_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'HR'\n",
    "df[target_column] = df[target_column].apply(lambda x: np.random.uniform(80,120 ) if x == \"na\" else x)\n",
    "\n",
    "# impute TEMP column\n",
    "# Replace 'na' strings with np.nan in column 'A'\n",
    "df['TEMP'] = df['TEMP'].replace('na', np.nan)\n",
    "\n",
    "# Impute missing values in column 'A' using the mean of column 'A'\n",
    "df['TEMP'] = df['TEMP'].fillna(df['TEMP'].mean())\n",
    "\n",
    "# # Save the updated DataFrame back to the same CSV file\n",
    "file_path = r'C:\\Users\\sahus\\Stress-Prediction\\Combined_data_withID\\New2\\final_csv.csv'\n",
    "df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 146453 entries, 0 to 146452\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   id      146453 non-null  object \n",
      " 1   EDA     146453 non-null  float64\n",
      " 2   TEMP    146453 non-null  object \n",
      " 3   BVP     146453 non-null  float64\n",
      " 4   HR      146453 non-null  object \n",
      "dtypes: float64(2), object(3)\n",
      "memory usage: 5.6+ MB\n"
     ]
    }
   ],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id      0.0\n",
       "EDA     0.0\n",
       "TEMP    0.0\n",
       "BVP     0.0\n",
       "HR      0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_count = (df == 'na').mean()\n",
    "na_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
